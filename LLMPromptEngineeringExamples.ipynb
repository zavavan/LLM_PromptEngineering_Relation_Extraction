{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0HFIVFK2amR5",
        "NMe6CltahGKf",
        "irQuoMIfoh7m"
      ],
      "authorship_tag": "ABX9TyM/JdBTquWMA7OCiwFxV9qn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zavavan/LLM_PromptEngineering_Relation_Extraction/blob/main/LLMPromptEngineeringExamples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq5BKXjeP1xM"
      },
      "source": [
        "## Initialize Hugging Face Models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soZW0jIrkQ3u"
      },
      "outputs": [],
      "source": [
        "!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U datasets scipy # ipywidgets\n",
        "!pip install -U bitsandbytes accelerate xformers einops\n",
        "!pip install --upgrade torch torchvision\n",
        "!pip install datasets\n",
        "!pip install evaluate \\\n",
        "    rouge_score \\\n",
        "    loralib \\\n",
        "    peft --quiet\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Crx-LoTM6zY",
        "outputId": "5b923bee-e921-424f-984f-17975118f305"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. Using GPU.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# Check if CUDA is available before importing cuda\n",
        "if torch.cuda.is_available():\n",
        "    from torch import cuda\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"CUDA is available. Using GPU.\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CUDA is not available. Using CPU.\")\n",
        "import evaluate\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM,  AutoModelForSeq2SeqLM, GenerationConfig, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQ-RDoDEVo27"
      },
      "outputs": [],
      "source": [
        "#!pip install openpyxl\n",
        "#!pip install XlsxWriter\n",
        "import regex as re\n",
        "import pickle\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import sys\n",
        "import time\n",
        "import csv\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import os\n",
        "import regex as re\n",
        "from string import punctuation\n",
        "import tqdm\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'; print(device)\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVTl9rslpLqb"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dntaLAySQe2L"
      },
      "outputs": [],
      "source": [
        "#model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "model_id = \"Open-Orca/Mistral-7B-OpenOrca\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXW-w8WUouLm"
      },
      "outputs": [],
      "source": [
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "load_in_4bit=True,\n",
        "bnb_4bit_use_double_quant=True,\n",
        "bnb_4bit_quant_type=\"nf4\",\n",
        "bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "model_id,\n",
        "trust_remote_code=True,\n",
        "quantization_config=bnb_config,\n",
        "device_map='auto',\n",
        ")\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "model_id,padding_side=\"left\"\n",
        ")\n",
        "if tokenizer.pad_token is None:\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "#model.generation_config.pad_token_id = tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HFIVFK2amR5"
      },
      "source": [
        "## LOAD HUGGING FACE DATASET\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imFMHPgZaoso",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "bbd02914-bd45-4b68-8e45-1db3974abdf2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'load_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ad1dadce3205>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhuggingface_dataset_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhuggingface_dataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
          ]
        }
      ],
      "source": [
        "huggingface_dataset_name = \"\"\n",
        "dataset = load_dataset(huggingface_dataset_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEPlSIISO03J"
      },
      "source": [
        "## LEARNING METHODS:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQI6mV4wFfkX"
      },
      "source": [
        "### ZERO-SHOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv0sXQXeb5nO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bc1b247-7725-4f25-c3d8-1b57fae8f858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given a text enclosed in triple quotese and a pair of entities E1 and E2, classify the relation holding between E1 and E2.\n",
            "The relations are identified with N labels from 0 to N-1. The meaning of the labels is the following:\n",
            "0 means that E1 ... E2\n",
            "1 ...\n",
            "N means that E1 and E2 are in none of the relations above.\n",
            "For the output use the format LABEL: X\n",
            "Text:'''Hyperglycemia : Patient is not known to be a diabetic and was felt [**12-26**] steroids , his sugars were controlled on sliding scale insulin in the hospital'''\n",
            "Entities: E1:'''Hyperglycemia''', E2:'''steroids'''\n"
          ]
        }
      ],
      "source": [
        "## this is a simple function to generate prompt tailored for Relaton CLassification on text. Modify according to your specific task\n",
        "def generate_relation_classification_prompt(input_text, e1, e2):\n",
        "\n",
        "  return  f\"\"\"Given a text enclosed in triple quotese and a pair of entities E1 and E2, classify the relation holding between E1 and E2.\n",
        "The relations are identified with N labels from 0 to N-1. The meaning of the labels is the following:\n",
        "0 means that E1 ... E2\n",
        "1 ...\n",
        "N means that E1 and E2 are in none of the relations above.\n",
        "For the output use the format LABEL: X\n",
        "Text:'''{input_text}'''\n",
        "Entities: E1:'''{e1}''', E2:'''{e2}'''\"\"\"\n",
        "\n",
        "\n",
        "## example with input sentence: \"Hyperglycemia : Patient is not known to be a diabetic and was felt [**12-26**] steroids , his sugars were controlled on sliding scale insulin in the hospital\"\n",
        "## and entities: e1='Hyperglycemia', e2='steroids'\n",
        "print(generate_relation_classification_prompt('Hyperglycemia : Patient is not known to be a diabetic and was felt [**12-26**] steroids , his sugars were controlled on sliding scale insulin in the hospital', 'Hyperglycemia', 'steroids'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## RUNNING ZERO-SHOT ON DATASET TEST PARTITION\n",
        "\n",
        "\n",
        "model_generated_labels_zero_shot = []  #store output labels\n",
        "\n",
        "\n",
        "## regex patterns to match response strings. Adapt according to output format specified in promp (expect non-compliance from LLMs response)\n",
        "answer_label_regex_pattern =  re.compile(r'LABEL:\\s?(\\d+)')\n",
        "answer_label_regex_pattern_1 =  re.compile(r'the label is\\s?(\\d+)')\n",
        "answer_label_regex_pattern_2 =  re.compile(r'The relation between E1 and E2 is\\s?(\\d+)')\n",
        "\n",
        "\n",
        "for i, example in enumerate(tqdm.tqdm(dataset['test'])):\n",
        "  input_text = example['Text']  ##reading the input sentence from dataset. To be modified according to the target dataset used\n",
        "  e1 = example['E1'] ## reading input entities from the dataset. To be adapted according to the target dataset\n",
        "  e2 = example['E2'] ## reading input entities from the dataset. To be adapted according to the target dataset\n",
        "  label = example['Label'] ## reading gold standard relation label from the dataset. To be adapted according to the target dataset\n",
        "  prompt = generate_relation_classification_prompt(input_text, e1, e2)\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt',padding=True#, add_special_tokens=False\n",
        "                      )\n",
        "\n",
        "  output = tokenizer.decode(\n",
        "      model.generate(\n",
        "          inputs[\"input_ids\"],\n",
        "          pad_token_id=tokenizer.pad_token_id,\n",
        "          max_new_tokens=200, # set according to task use case\n",
        "      )[0],\n",
        "      skip_special_tokens=True\n",
        "  )\n",
        "\n",
        "  ## removing prompt text and new lines from response\n",
        "  processed_output = output.replace(prompt,'')\n",
        "  processed_output = re.sub('\\n|\\r','',processed_output)\n",
        "\n",
        "  # matching answer regex patterns\n",
        "  answer_match = answer_label_regex_pattern.search(processed_output)\n",
        "  answer_match_1 = answer_label_regex_pattern_1.search(processed_output)\n",
        "  answer_match_2 = answer_label_regex_pattern_2.search(processed_output)\n",
        "\n",
        "  if answer_match:\n",
        "    temp_label = answer_match.group(1)\n",
        "    model_generated_labels_zero_shot.append(temp_label)\n",
        "  elif answer_match_1:\n",
        "    temp_label = answer_match_1.group(1)\n",
        "    model_generated_labels_zero_shot.append(temp_label)\n",
        "  elif answer_match_2:\n",
        "    temp_label = answer_match_2.group(1)\n",
        "    model_generated_labels_zero_shot.append(temp_label)\n",
        "  else:\n",
        "    # define a default label assignment if response does not match expected format\n",
        "    model_generated_labels_zero_shot.append('N')\n",
        "\n",
        "  ## DEBUGGING OUTPUT (adapt accordingly):\n",
        "  print('-'.join('' for x in range(100)))\n",
        "  print('Example ', i + 1)\n",
        "  print('-'.join('' for x in range(100)))\n",
        "  print(f'INPUT PROMPT:\\n{prompt}')\n",
        "  print('-'.join('' for x in range(100)))\n",
        "  print(f'GOLD STANDARD LABEL\\n{label}')\n",
        "  print('-'.join('' for x in range(100)))\n",
        "  print(f'PROCESSED OUTPUT:\\n{processed_output}\\n')\n",
        "  print('-'.join('' for x in range(100)))"
      ],
      "metadata": {
        "id": "ylapt0YsanJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ### Chain Of Thought Prompt"
      ],
      "metadata": {
        "id": "NMe6CltahGKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Modify according to your specific task\n",
        "def generate_relation_classification_CoT_prompt_0(input_text, e1, e2):\n",
        "\n",
        "  return  f\"\"\"\n",
        "Given a text enclosed in triple quotes and a pair of entities E1 and E2 enclosed in triple quotes, what is the relation holding between E1 and E2?\n",
        "Text: '''{input_text}'''.\n",
        "E1: '''{e1}''', E2: '''{e2}'''\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "## sample function taking as input the LLM response of a first prompt interaction \"processed_output_0\". Modify according to your specific task\n",
        "def generate_relation_classification_CoT_prompt_1(processed_output_0, e1, e2):\n",
        "\n",
        "  return  f\"\"\"\n",
        "Now, given the following relation between the entities E1:'''{e1}''' and E2: '''{e2}'''\n",
        "Relation: {processed_output_0}\n",
        "which of the following labels from 0 to 8 best describes it? The meaning of the labels is the following:\n",
        "0 means that '''{e1}''' causes '''{e2}'''\n",
        "...\n",
        "8 means that '''{e1}''' and '''{e2}''' are in none of the relations above.\n",
        "For the output use the format LABEL: X\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4GvT7dZWihGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAqAEubqYab7"
      },
      "outputs": [],
      "source": [
        "model_generated_labels_zero_shot_CoT = []\n",
        "\n",
        "\n",
        "## regex patterns to match response strings. Adapt according to output format specified in promp (expect non-compliance from LLMs response)\n",
        "answer_label_regex_pattern =  re.compile(r'LABEL:\\s?(\\d+)')\n",
        "answer_label_regex_pattern_1 =  re.compile(r'the label is\\s?(\\d+)')\n",
        "answer_label_regex_pattern_2 =  re.compile(r'The relation between E1 and E2 is\\s?(\\d+)')\n",
        "\n",
        "for i, example in enumerate(tqdm.tqdm(dataset['test'])):\n",
        "\n",
        "  input_text = example['Text']  ##reading the input sentence from dataset. To be modified according to the target dataset used\n",
        "  e1 = example['E1']  ## reading input entities from the dataset. To be adapted according to the target dataset\n",
        "  e2 = example['E2']  ## reading input entities from the dataset. To be adapted according to the target dataset\n",
        "  gold_standard_label = example['Label']  ## reading gold standard relation label from the dataset. To be adapted according to the target dataset\n",
        "\n",
        "  # first interaction\n",
        "  prompt_CoT_0 = generate_relation_classification_CoT_prompt_0(input_text, e1, e2)\n",
        "\n",
        "  inputs = tokenizer(prompt_CoT_0, return_tensors='pt',padding=True)\n",
        "  output_0 = tokenizer.decode(\n",
        "      model.generate(\n",
        "          inputs[\"input_ids\"],\n",
        "          pad_token_id=tokenizer.pad_token_id,\n",
        "          max_new_tokens=200, ## adapt according to use case task\n",
        "      )[0],\n",
        "      skip_special_tokens=True\n",
        "  )\n",
        "\n",
        "  ## removing prompt text and new lines from response\n",
        "  processed_output_0= output_0.replace(prompt_CoT_0,'')\n",
        "  processed_output_0 = re.sub('\\n|\\r','',processed_output_0)\n",
        "\n",
        "\n",
        "  # second interaction\n",
        "  prompt_CoT_1 = generate_relation_classification_CoT_prompt_1(processed_output_0, e1, e2):\n",
        "\n",
        "  inputs = tokenizer(prompt_CoT_1, return_tensors='pt',padding=True)\n",
        "  output = tokenizer.decode(\n",
        "      model.generate(\n",
        "          inputs[\"input_ids\"],\n",
        "          pad_token_id=tokenizer.pad_token_id,\n",
        "          max_new_tokens=50, ## adapt according to use case task\n",
        "      )[0],\n",
        "      skip_special_tokens=True\n",
        "  )\n",
        "\n",
        "  ## removing prompt text and new lines from response\n",
        "  processed_output= output.replace(prompt_CoT_1,'')\n",
        "  processed_output = re.sub('\\n|\\r','',processed_output)\n",
        "\n",
        "\n",
        "  answer_match = answer_label_regex_pattern.search(processed_output)\n",
        "  answer_match_1 = answer_label_regex_pattern_1.search(processed_output)\n",
        "  answer_match_2 = answer_label_regex_pattern_2.search(processed_output)\n",
        "\n",
        "  if answer_match:\n",
        "    model_generated_labels_zero_shot_CoT.append(answer_match.group(1))\n",
        "  elif answer_match_1:\n",
        "    model_generated_labels_zero_shot_CoT.append(answer_match_1.group(1))\n",
        "  elif answer_match_2:\n",
        "    model_generated_labels_zero_shot_CoT.append(answer_match_2.group(1))\n",
        "  # define a default label assignment if response does not match expected format\n",
        "  else:\n",
        "    model_generated_labels_zero_shot_CoT.append('N')\n",
        "\n",
        "  ## DEBUGGING OUTPUT (adapt accordingly):\n",
        "  print('-'.join('' for x in range(100)))\n",
        "  print('Example ', i + 1)\n",
        "  print('-'.join('' for x in range(100)))\n",
        "  print('GOLD STANDARD LABEL: ' + str(gold_standard_label))\n",
        "  print('-'.join('' for x in range(100)))\n",
        "  print(f'CoT PROMPT 0:\\n{prompt_CoT_0}')\n",
        "  print('-'.join('' for x in range(100)))\n",
        "  print(f'CoT PROMPT 0 MODEL RESPONSE:\\n{processed_output_0}\\n')\n",
        "  print('-'.join('' for x in range(100)))\n",
        "  print(f'CoT PROMPT 1:\\n{prompt_CoT_1}')\n",
        "  print('-'.join('' for x in range(100)))\n",
        "  print(f'CoT PROMPT 1 MODEL RESPONSE:\\n{processed_output}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FEW-SHOT"
      ],
      "metadata": {
        "id": "irQuoMIfoh7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function that generates n-shot examples from the training partition of a given HuggingFace dataset\n",
        "\n",
        "def generate_n_shot_examples_relation_labels(dataset,n):\n",
        "  n_shot_example_string = ''\n",
        "  sampled_examples = random.sample(list(dataset['train']), n)\n",
        "  for i, example in enumerate(sampled_examples):\n",
        "    if i < n:\n",
        "      n_shot_example_string=n_shot_example_string + \"Text:'''\" + example['Text'] + \"'''\\n\"\n",
        "      n_shot_example_string=n_shot_example_string + \"Entities: E1:'''\" + str(example['E1']) + \"''', E2:'''\" + str(example['E2']) + \"'''\\n\"\n",
        "      n_shot_example_string=n_shot_example_string + 'LABEL: ' + str(example['Label']) + '\\n'\n",
        "  return n_shot_example_string"
      ],
      "metadata": {
        "id": "KuNcal4FojjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## this is a simple function to generate prompt tailored for Relaton CLassification on text. Modify according to your specific task\n",
        "def generate_relation_classification_prompt_instructions():\n",
        "  return  f\"\"\"Given a text enclosed in triple quotese and a pair of entities E1 and E2, classify the relation holding between E1 and E2.\n",
        "The relations are identified with N labels from 0 to N-1. The meaning of the labels is the following:\n",
        "0 means that E1 ... E2\n",
        "1 ...\n",
        "N means that E1 and E2 are in none of the relations above.\n",
        "For the output use the format LABEL: X\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "def generate_relation_classification_prompt_input_part(input_text,e1,e2):\n",
        "  return f\"\"\"Text: '''{input_text}'''\n",
        "E1: '''{e1}''', E2: '''{e2}'''\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7GbryKw5vH2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## function for running relation classification using few-shot approach (with \"num_examples\" examples) and returning a list of parsed result labels\n",
        "\n",
        "def run_few_shot_n_examples_re_classification(num_examples):\n",
        "  model_generated_labels_few_shot = []\n",
        "\n",
        "  prompt_instructions = generate_relation_classification_prompt_instructions()\n",
        "  for i, example in enumerate(tqdm.tqdm(dataset['test'])):\n",
        "\n",
        "    #the few shot example are generated randomly for each text instance, to mitigate the impact of choosing a set of poor examples in the prompt:\n",
        "    n_shot_examples = generate_n_shot_examples_relation_labels(dataset,num_examples)\n",
        "\n",
        "    input_text = example['Text']\n",
        "    e1 = example['E1']\n",
        "    e2 = example['E2']\n",
        "    gold_standard_label = example['Label']\n",
        "    prompt = f\"\"\"{prompt_instructions}\n",
        "{n_shot_examples}\n",
        "{generate_relation_classification_prompt_input_part(input_text,e1,e2)}\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors='pt',padding=True)\n",
        "\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            max_new_tokens=20,  # set according to task use case\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    ## removing prompt text and new lines from response\n",
        "    processed_output = output.replace(prompt,'')\n",
        "    processed_output = re.sub('\\n|\\r','',processed_output)\n",
        "\n",
        "    answer_match = answer_label_regex_pattern.search(processed_output)\n",
        "    answer_match_1 = answer_label_regex_pattern_1.search(processed_output)\n",
        "    answer_match_2 = answer_label_regex_pattern_2.search(processed_output)\n",
        "\n",
        "    if answer_match:\n",
        "      model_generated_labels_few_shot.append(answer_match.group(1))\n",
        "    elif answer_match_1:\n",
        "      model_generated_labels_few_shot.append(answer_match_1.group(1))\n",
        "    elif answer_match_2:\n",
        "      model_generated_labels_few_shot.append(answer_match_2.group(1))\n",
        "    else:\n",
        "      model_generated_labels_few_shot.append('N')\n",
        "\n",
        "    print('-'.join('' for x in range(100)))\n",
        "    print('Example ', i + 1)\n",
        "    print('-'.join('' for x in range(100)))\n",
        "    print(f'INPUT PROMPT:\\n{prompt}')\n",
        "    print('-'.join('' for x in range(100)))\n",
        "    print(f'GOLD STANDARD LABEL:\\n{gold_standard_label}')\n",
        "    print('-'.join('' for x in range(100)))\n",
        "    print(f'MODEL GENERATION - FEW SHOT:\\n{output}\\n')\n",
        "    print('-'.join('' for x in range(100)))\n",
        "    print(f'PROCESSED OUTPUT:\\n{processed_output}\\n')\n",
        "    print('-'.join('' for x in range(100)))\n",
        "  return model_generated_labels_few_shot"
      ],
      "metadata": {
        "id": "L08Wbl49saRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_generated_labels_few_shot = run_few_shot_n_examples_re_classification(5)"
      ],
      "metadata": {
        "id": "wc4GVCv-xLpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Yj3HQxocX8a"
      },
      "source": [
        "## EVALUATION:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHU37PVUcaTa"
      },
      "outputs": [],
      "source": [
        "metricAccuracy = evaluate.load(\"accuracy\")\n",
        "metricF1 = evaluate.load(\"f1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ppP-_GXKXmy"
      },
      "outputs": [],
      "source": [
        "gs_labels = []\n",
        "\n",
        "for i, example in enumerate(dataset['test']):\n",
        "  gs_labels.append(example['Label'])\n",
        "\n",
        "accuracy_results = metricAccuracy.compute(predictions=model_generated_labels,\n",
        "    references=gs_labels)\n",
        "print(accuracy_results)\n",
        "\n",
        "fscore_results = metricF1.compute(predictions=model_generated_labels,\n",
        "    references=gs_labels, average='macro')\n",
        "print(fscore_results)"
      ]
    }
  ]
}